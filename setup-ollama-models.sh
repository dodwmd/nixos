#!/usr/bin/env bash
# Setup script for optimal LLM models on RTX 4080 (16GB VRAM)

set -e

echo "ğŸš€ Setting up Ollama models for RTX 4080..."

# Primary coding model - excellent for agentic tasks
echo "ğŸ“¦ Pulling Qwen2.5-Coder 14B (Primary coding model)..."
ollama pull qwen2.5-coder:14b

# Alternative: Google's latest general model
echo "ğŸ“¦ Pulling Gemma 3N 27B (Google's latest, general purpose)..."
ollama pull gemma2:27b

# Backup coding model - smaller, faster
echo "ğŸ“¦ Pulling Qwen2.5-Coder 7B (Backup coding model)..."
ollama pull qwen2.5-coder:7b

# General purpose model for reasoning
echo "ğŸ“¦ Pulling Llama 3.1 8B (General reasoning)..."
ollama pull llama3.1:8b

# Specialized models
echo "ğŸ“¦ Pulling DeepSeek-Coder 6.7B (Code completion)..."
ollama pull deepseek-coder:6.7b

# Small utility model for quick tasks
echo "ğŸ“¦ Pulling Phi-3 Mini (Quick tasks)..."
ollama pull phi3:mini

echo "âœ… Model setup complete!"
echo ""
echo "ğŸ’¡ Recommended usage:"
echo "  - qwen2.5-coder:14b  â†’ Primary agentic coding (8-10GB VRAM)"
echo "  - qwen2.5-coder:7b   â†’ Fast coding tasks (4-6GB VRAM)"
echo "  - llama3.1:8b        â†’ General reasoning (4-6GB VRAM)"
echo "  - deepseek-coder:6.7b â†’ Code completion (3-4GB VRAM)"
echo "  - phi3:mini          â†’ Quick queries (1-2GB VRAM)"
echo ""
echo "ğŸ”§ Your RTX 4080 (16GB) can run the 14B model comfortably!"
echo "ğŸ¯ OpenCode is configured to use qwen2.5-coder:14b as primary model"
echo "â˜ï¸  Claude Sonnet 3.5 will be used as fallback for complex reasoning"
